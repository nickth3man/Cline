Okay, I understand. You've provided a set of files representing a project that downloads YouTube videos and processes them (currently focused on transcription and summarization using OpenRouter), along with context files suggesting a more advanced transcription workflow involving local diarization (`pyannote.audio`) and LLM-based correction.

It seems there are two related but distinct functionalities described:

1.  **The GUI Application (`main.py`):** Downloads YouTube videos, transcribes using OpenRouter (likely Whisper), summarizes using OpenRouter (selected LLM), and saves results. It uses `yt-dlp` and `requests`.
2.  **The Advanced Transcription Workflow (`transcription_workflow.py`):** Takes an audio file, transcribes (Whisper via OpenRouter), diarizes locally (`pyannote.audio`), and uses an LLM (via OpenRouter) to *correct and format* the transcript based on diarization. It uses the `openai` library client and `pyannote.audio`.

**Assumption:** You want to **integrate the advanced transcription workflow from `transcription_workflow.py` into the YouTube downloader GUI application (`main.py`)**, replacing the current simpler transcription/summarization logic with transcription -> diarization -> correction. You also want **unit tests** for the core logic.

**Plan:**

1.  **Refactor `transcription_workflow.py`:** Make it more library-like, removing the hardcoded API key and `if __name__ == "__main__"` block, ensuring functions are easily importable and use environment variables for secrets (`OPENROUTER_API_KEY`, `HF_TOKEN`). Add proper `ffmpeg` handling.
2.  **Modify `main.py`'s `DownloadThread`:**
    *   Integrate the `extract_or_convert_audio` function (requires `ffmpeg`).
    *   Replace the `requests`-based `transcribe_audio` with the `openai`-client-based one from the refactored workflow.
    *   Add calls to the `diarize_speakers` function.
    *   Replace the `summarize_text` call with the `correct_transcript` function, passing the transcript and diarization results.
    *   Update status reporting (`video_status_update`) for the new steps (e.g., "Converting Audio", "Diarizing", "Correcting Transcript").
    *   Ensure robust error handling for each new step.
3.  **Update `main.py` GUI (Minor):** Maybe adjust the model selection label if it's now for correction, not summarization, or add separate configuration if needed. Ensure the API key input is used correctly by the `openai` client setup.
4.  **Create `requirements.txt`:** List all necessary dependencies (`PyQt6`, `yt-dlp`, `openai`, `pyannote.audio`, `torch`, `torchaudio`, `python-dotenv`, `pytest`).
5.  **Create Unit Tests (`tests/test_workflow.py`):** Write `pytest` tests mocking external dependencies (`yt-dlp`, OpenRouter API calls, `pyannote.audio`, `ffmpeg`) to test the core logic functions.
6.  **Provide Setup Instructions:** Explain environment variables, `ffmpeg` installation, `pyannote` model agreements/token, and running the app/tests.

**Security Warning:** The provided `transcription_workflow.py` **hardcodes an API key**. This is a major security risk. The generated code below will assume the key is loaded from an environment variable `OPENROUTER_API_KEY`. **You MUST remove the hardcoded key from your actual files and use environment variables.** Similarly, the Hugging Face token for `pyannote` should come from `HF_TOKEN`.

Let's generate the updated code and tests.

---

**1. Refactored Workflow Logic (e.g., save as `workflow_logic.py`)**

```python
# --- START OF FILE workflow_logic.py ---

import os
import subprocess
import logging
import datetime
import traceback
from openai import OpenAI
from pyannote.audio import Pipeline
import torch
from dotenv import load_dotenv

# --- Load Environment Variables ---
load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
HF_TOKEN = os.getenv("HF_TOKEN") # Hugging Face token for pyannote models

# --- Configuration & Initialization ---
# Configure OpenAI client for OpenRouter
if OPENROUTER_API_KEY:
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
        # Optional headers for OpenRouter tracking
        # default_headers={"HTTP-Referer": "YOUR_APP_URL", "X-Title": "YOUR_APP_NAME"}
    )
    logging.info("OpenAI client configured for OpenRouter.")
else:
    client = None
    logging.warning("OPENROUTER_API_KEY not found in environment variables. API calls will fail.")

# Load Diarization pipeline
diarization_pipeline = None
try:
    if HF_TOKEN is None:
        logging.warning("HF_TOKEN environment variable not set. Trying to load pyannote pipeline without auth token. This may fail for gated models.")

    # Check for GPU availability
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info(f"Using device: {device} for pyannote.audio")

    # Use a specific pipeline version for reproducibility if desired
    # Ensure you have accepted the terms on Hugging Face for this model.
    diarization_pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=HF_TOKEN # Pass token or True if logged in via CLI, None otherwise
    )
    diarization_pipeline.to(torch.device(device))
    logging.info("Pyannote diarization pipeline loaded successfully.")
except Exception as e:
    logging.error(f"Failed to load pyannote pipeline: {e}. Diarization will be skipped.", exc_info=True)
    diarization_pipeline = None # Indicate failure


# --- Helper Functions ---

def check_ffmpeg():
    """Checks if ffmpeg is accessible."""
    try:
        subprocess.run(["ffmpeg", "-version"], check=True, capture_output=True)
        logging.info("FFmpeg found.")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        logging.warning("FFmpeg not found or not executable. Audio extraction/conversion might fail.")
        return False

_ffmpeg_checked = False
_ffmpeg_present = False

def extract_or_convert_audio(input_path, output_wav_path):
    """
    Extracts audio from video or converts to WAV (16kHz mono) using FFmpeg.
    Raises RuntimeError if FFmpeg fails or is not found (unless input is already compliant).
    """
    global _ffmpeg_checked, _ffmpeg_present
    if not _ffmpeg_checked:
        _ffmpeg_present = check_ffmpeg()
        _ffmpeg_checked = True

    # Basic check if input is already WAV (can be improved with ffprobe/sox)
    # For now, we always try to convert to ensure 16kHz mono PCM s16le
    # if input_path.lower().endswith(".wav"):
    #     logging.info(f"Input {input_path} is WAV, but converting to ensure 16khz Mono.")
        # Could add a check here to see if conversion is *really* needed

    if not _ffmpeg_present:
        raise RuntimeError("FFmpeg is required for audio preparation but was not found.")

    logging.info(f"Converting/Extracting audio to 16kHz mono WAV: {input_path} -> {output_wav_path}")
    command = [
        "ffmpeg",
        "-i", input_path,
        "-vn",          # No video
        "-acodec", "pcm_s16le", # Standard WAV codec
        "-ac", "1",     # Mono channel
        "-ar", "16000", # 16kHz sample rate
        "-nostdin",     # Prevent interference with stdin (good practice)
        "-y",           # Overwrite output file if it exists
        output_wav_path
    ]
    try:
        process = subprocess.run(command, check=True, capture_output=True)
        logging.info(f"FFmpeg conversion successful: {output_wav_path}")
        logging.debug(f"FFmpeg stderr: {process.stderr.decode(errors='ignore')}")
        return output_wav_path
    except subprocess.CalledProcessError as e:
        error_message = f"FFmpeg failed for {input_path}: {e.stderr.decode(errors='ignore')}"
        logging.error(error_message)
        raise RuntimeError(error_message)
    except FileNotFoundError:
         # This case should be caught by check_ffmpeg(), but as a fallback:
         logging.error("FFmpeg command not found during execution.")
         raise RuntimeError("FFmpeg command not found. Please ensure FFmpeg is installed and in your PATH.")
    except Exception as e:
        # Catch unexpected errors during subprocess execution
        error_message = f"An unexpected error occurred during FFmpeg execution for {input_path}: {str(e)}"
        logging.error(error_message, exc_info=True)
        raise RuntimeError(error_message)


# --- Core Workflow Functions ---

def transcribe_audio(audio_path: str) -> str:
    """
    Transcribes the given audio file using Whisper via OpenRouter API.
    Requires OPENROUTER_API_KEY env var.
    """
    if not client:
        raise ConnectionError("OpenAI client not initialized. Check OPENROUTER_API_KEY.")
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    logging.info(f"Starting transcription for {audio_path} via OpenRouter...")
    try:
        with open(audio_path, "rb") as audio_file:
            # Ensure timeout is reasonable for potentially large files
            transcript = client.with_options(timeout=600.0).audio.transcriptions.create(
                model="openai/whisper-large-v3", # Or configure model choice
                file=audio_file
                # Add other parameters like 'language' or 'prompt' if needed
            )
        logging.info(f"Transcription successful for {audio_path}.")
        return transcript.text
    except Exception as e:
        logging.error(f"OpenRouter transcription API error for {audio_path}: {e}", exc_info=True)
        # Re-raise a more specific error if possible, or the original one
        raise RuntimeError(f"Transcription failed: {e}") from e

def diarize_speakers(audio_path: str) -> list:
    """
    Performs speaker diarization using the local pyannote.audio pipeline.
    Requires pyannote.audio and models to be set up. HF_TOKEN might be needed.
    Returns a list of segments: [{"speaker": label, "start": secs, "end": secs}, ...]
    Returns empty list if pipeline failed to load or diarization fails.
    """
    if not diarization_pipeline:
         logging.warning(f"Diarization pipeline not loaded. Skipping diarization for {audio_path}.")
         return []
    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found for diarization: {audio_path}")

    logging.info(f"Starting speaker diarization for {audio_path}...")
    try:
        # Apply pipeline with default hyperparameters
        # You might need to adjust `min_duration_on`, `min_duration_off` for specific audio
        diarization = diarization_pipeline(audio_path, num_speakers=None) # Let pipeline detect num_speakers

        segments = []
        # Convert pyannote annotation to a simpler list format with float seconds
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "speaker": speaker, # e.g., SPEAKER_00, SPEAKER_01
                "start": round(turn.start, 3), # Round to milliseconds
                "end": round(turn.end, 3)
            })

        # Sort segments by start time as pyannote guarantees it, but good practice
        segments.sort(key=lambda x: x["start"])
        logging.info(f"Diarization successful for {audio_path}. Found {len(segments)} segments for {len(diarization.labels())} speakers.")
        return segments
    except Exception as e:
        logging.error(f"Pyannote diarization error for {audio_path}: {e}", exc_info=True)
        return [] # Return empty list on error to allow pipeline to continue

def format_diarization_for_llm(diarization_result: list) -> str:
    """Formats diarization results into a string suitable for an LLM prompt."""
    if not diarization_result:
        return "Speaker diarization information is not available.\n"

    output = "Speaker Turns (Format: SPEAKER_ID StartTime[H:MM:SS.ms]-EndTime[H:MM:SS.ms]):\n"
    for segment in diarization_result:
        # Format timestamps precisely
        start_td = datetime.timedelta(seconds=segment['start'])
        end_td = datetime.timedelta(seconds=segment['end'])
        # Pad microseconds to 3 digits (milliseconds)
        start_str = f"{str(start_td).split('.')[0]}.{str(start_td.microseconds // 1000).zfill(3)}"
        end_str = f"{str(end_td).split('.')[0]}.{str(end_td.microseconds // 1000).zfill(3)}"

        output += f"- {segment['speaker']} {start_str}-{end_str}\n"
    return output

def correct_transcript(raw_transcript: str, diarization_result: list, correction_model: str = "anthropic/claude-3-haiku-20240307") -> str:
    """
    Corrects the transcript using an LLM via OpenRouter, incorporating diarization info.
    Requires OPENROUTER_API_KEY env var.
    """
    if not client:
        raise ConnectionError("OpenAI client not initialized. Check OPENROUTER_API_KEY.")
    if not raw_transcript:
        logging.warning("Raw transcript is empty, cannot perform correction.")
        return ""

    logging.info(f"Starting transcript correction using model {correction_model} via OpenRouter...")

    # Format diarization data for the prompt
    speaker_turns_text = format_diarization_for_llm(diarization_result)

    # --- Develop the Prompt Template ---
    # This prompt is crucial and likely needs refinement based on testing results.
    prompt_template = f"""You are an expert transcript editor. Your task is to take a raw, unprocessed transcript generated by an ASR system and speaker turn information generated by a diarization system, and produce a clean, accurate, and highly readable final transcript.

**Input:**
1.  **Speaker Turns:** A list indicating which speaker (e.g., SPEAKER_00, SPEAKER_01) was speaking during specific time intervals.
2.  **Raw Transcript:** The potentially inaccurate output from the ASR system.

**Instructions:**
1.  **Integrate Speaker Labels:** Use the "Speaker Turns" data to assign the correct speaker label (e.g., "SPEAKER_00:", "SPEAKER_01:") to the corresponding speech segments in the transcript. Start each new speaker's turn on a new line, preceded by their label.
2.  **Correct ASR Errors:** Fix spelling mistakes, grammatical errors, and punctuation inaccuracies commonly found in raw ASR output. Pay close attention to capitalization (start of sentences, proper nouns).
3.  **Improve Readability:** Break down long paragraphs into shorter ones where appropriate. Ensure smooth transitions between speaker turns. Remove filler words (um, uh, like) *only if* they significantly detract from readability, but generally preserve the natural flow of speech.
4.  **Handle Overlap/Uncertainty (Implicitly):** The raw transcript and speaker turns might not align perfectly. Use your best judgment to segment the text according to the speaker turn information, even if the exact word boundaries are slightly off. If the diarization is empty or clearly wrong, format the text as a single block without speaker labels.
5.  **Accuracy is Key:** Preserve the original meaning. Do not add information not present in the raw transcript.
6.  **Output Format:** Produce *only* the final, corrected, speaker-labeled transcript text. Do NOT include the speaker turn list, timestamps, or any other meta-commentary in the final output.

**Speaker Turns:**
{speaker_turns_text}

**Raw Transcript:**
{raw_transcript}

**Corrected and Formatted Transcript:**
"""

    try:
        logging.info(f"Sending correction request to OpenRouter model: {correction_model}")
        # Use a model known for strong instruction following and text generation.
        # Increase timeout for potentially long correction tasks
        chat_completion = client.with_options(timeout=300.0).chat.completions.create(
            model=correction_model, # e.g., Claude 3, GPT-4
            messages=[
                # System message can help set the persona more strongly
                {"role": "system", "content": "You are an expert transcript editor focused on accuracy and readability."},
                {"role": "user", "content": prompt_template}
            ],
            temperature=0.5, # Lower temperature for more deterministic corrections
            # max_tokens= ... # Consider setting if needed, but often inferred
        )
        corrected_transcript = chat_completion.choices[0].message.content.strip()
        logging.info("Transcript correction successful.")
        return corrected_transcript
    except Exception as e:
        logging.error(f"OpenRouter correction API error: {e}", exc_info=True)
        raise RuntimeError(f"Transcript correction failed: {e}") from e

# --- END OF FILE workflow_logic.py ---
```

**2. Updated `main.py`**

```python
# --- START OF FILE main.py ---

# main.py - YouTube Playlist Processor with PyQt6 GUI and Advanced Workflow

import sys
import threading
import os
import traceback
import logging
import re
import json # Keep json import for potential future use or debugging
from PyQt6.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout,
                             QHBoxLayout, QLabel, QLineEdit, QComboBox,
                             QPushButton, QProgressBar, QTableWidget,
                             QTableWidgetItem, QTextEdit, QMessageBox,
                             QFileDialog, QGridLayout) # Added QGridLayout
from PyQt6.QtCore import QThread, pyqtSignal, Qt, QTimer # Added QTimer for periodic checks
from dotenv import load_dotenv
import yt_dlp

# Import the refactored workflow logic
import workflow_logic

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(threadName)s] - %(message)s')
# Load .env file for API keys if it exists
load_dotenv()

def sanitize_filename(filename):
    """Removes invalid characters for filenames/paths."""
    sanitized = re.sub(r'[^\w\-. ]', '', filename) # Allow dots and hyphens
    sanitized = re.sub(r'\s+', '_', sanitized) # Replace whitespace sequences with single underscore
    return sanitized[:150] # Limit length further

class ProcessingThread(QThread):
    # Signals for UI updates
    status_update = pyqtSignal(str)
    video_status_update = pyqtSignal(int, str, str)  # video_index, status, details
    error_signal = pyqtSignal(int, str)  # video_index (-1 for general), error_message
    progress_update = pyqtSignal(int, int)  # current_video_count, total_videos
    finished_signal = pyqtSignal() # Signal emitted when run() completes

    def __init__(self, playlist_url, resolution, output_path, correction_model):
        super().__init__()
        self.playlist_url = playlist_url
        self.resolution = resolution
        self.output_path = output_path
        self.correction_model = correction_model
        # API Keys are read from environment in workflow_logic.py
        self.video_list = []
        self._is_running = True # Use underscore convention for internal state
        self.current_operation_lock = threading.Lock() # To manage stopping gracefully
        self.current_operation = "" # Track current long operation
        logging.info("ProcessingThread initialized.")

    def run(self):
        logging.info("ProcessingThread started.")
        total_videos = 0
        processed_count = 0

        try:
            # --- Fetch Playlist Info ---
            if not self._is_running: return
            self.set_operation("Fetching playlist info...")
            self.status_update.emit(self.current_operation)
            ydl_opts_info = {'quiet': True, 'extract_flat': 'in_playlist', 'skip_download': True}
            try:
                with yt_dlp.YoutubeDL(ydl_opts_info) as ydl:
                    logging.info(f"Extracting info for playlist: {self.playlist_url}")
                    info = ydl.extract_info(self.playlist_url, download=False)
                    if 'entries' in info and info['entries']:
                        self.video_list = info['entries']
                        total_videos = len(self.video_list)
                        logging.info(f"Fetched {total_videos} videos.")
                        self.status_update.emit(f"Fetched {total_videos} videos. Starting processing...")
                        self.progress_update.emit(0, total_videos) # Initialize progress
                    else:
                        raise ValueError("No videos found in the playlist or playlist is invalid.")
            except Exception as e:
                 # Catch yt-dlp specific or general errors during info extraction
                 error_msg = f"Failed to extract playlist info: {str(e)}"
                 logging.error(f"{error_msg}\n{traceback.format_exc()}")
                 self.error_signal.emit(-1, error_msg)
                 return # Stop processing if playlist fetch fails
            finally:
                self.clear_operation()

            # --- Process Each Video ---
            for index, video_entry in enumerate(self.video_list):
                if not self._is_running:
                    logging.info("Processing stopped by user request.")
                    break

                processed_count = index # Track completed videos for progress bar
                video_title = video_entry.get('title', f'video_{index}')
                video_url = video_entry.get('webpage_url', video_entry.get('id')) # Use webpage_url reliably
                sanitized_title = sanitize_filename(video_title)
                video_folder_path = os.path.join(self.output_path, f"{index:03d}_{sanitized_title}") # Pad index
                download_path = os.path.join(video_folder_path, f"{sanitized_title}.mp4") # Assume mp4 download
                wav_audio_path = os.path.join(video_folder_path, f"{sanitized_title}_audio.wav")
                raw_transcript_path = os.path.join(video_folder_path, f"{sanitized_title}_raw_transcript.txt")
                diarization_path = os.path.join(video_folder_path, f"{sanitized_title}_diarization.json")
                corrected_transcript_path = os.path.join(video_folder_path, f"{sanitized_title}_corrected.md")

                logging.info(f"--- Processing video {index + 1}/{total_videos}: {video_title} ---")
                self.video_status_update.emit(index, "Preparing", video_title) # Initial status

                try:
                    # Create video-specific directory
                    if not self._is_running: break
                    self.set_operation(f"Creating directory for {video_title}")
                    os.makedirs(video_folder_path, exist_ok=True)
                    logging.info(f"Ensured directory exists: {video_folder_path}")
                    self.clear_operation()

                    # --- 1. Download Video ---
                    if not self._is_running: break
                    self.set_operation(f"Downloading {video_title}")
                    self.video_status_update.emit(index, "Downloading", "")
                    try:
                        ydl_opts_download = {
                            'format': f'bestvideo[height<={self.resolution}][ext=mp4]+bestaudio[ext=m4a]/best[height<={self.resolution}][ext=mp4]/best',
                            'outtmpl': download_path, # Use the specific path
                            'quiet': True,
                            'noprogress': True,
                            'postprocessors': [{ 'key': 'FFmpegVideoConvertor', 'preferedformat': 'mp4'}],
                             # Add user agent to potentially avoid throttling
                            'http_headers': {'User-Agent': 'Mozilla/5.0'},
                        }
                        logging.info(f"Starting download for: {video_url} to {download_path}")
                        with yt_dlp.YoutubeDL(ydl_opts_download) as ydl_download:
                            ydl_download.download([video_url])
                        logging.info(f"Download complete: {download_path}")
                    except Exception as e:
                         raise RuntimeError(f"Download failed: {str(e)}") # Wrap error
                    finally:
                        self.clear_operation()

                    # --- 2. Extract/Convert Audio ---
                    if not self._is_running: break
                    self.set_operation(f"Converting audio for {video_title}")
                    self.video_status_update.emit(index, "Converting Audio", "")
                    try:
                        workflow_logic.extract_or_convert_audio(download_path, wav_audio_path)
                    except Exception as e:
                        raise RuntimeError(f"Audio conversion failed: {str(e)}")
                    finally:
                        self.clear_operation()

                    # --- 3. Transcribe Audio ---
                    if not self._is_running: break
                    self.set_operation(f"Transcribing {video_title}")
                    self.video_status_update.emit(index, "Transcribing", "")
                    try:
                        raw_transcript = workflow_logic.transcribe_audio(wav_audio_path)
                        with open(raw_transcript_path, "w", encoding='utf-8') as f:
                             f.write(raw_transcript)
                        logging.info(f"Raw transcript saved: {raw_transcript_path}")
                    except Exception as e:
                         raise RuntimeError(f"Transcription failed: {str(e)}")
                    finally:
                        self.clear_operation()

                    # --- 4. Diarize Speakers ---
                    if not self._is_running: break
                    # Check if pipeline loaded - skip if not
                    if workflow_logic.diarization_pipeline:
                        self.set_operation(f"Diarizing speakers for {video_title}")
                        self.video_status_update.emit(index, "Diarizing", "")
                        try:
                            diarization_result = workflow_logic.diarize_speakers(wav_audio_path)
                            with open(diarization_path, "w", encoding='utf-8') as f:
                                json.dump(diarization_result, f, indent=2)
                            logging.info(f"Diarization results saved: {diarization_path}")
                        except Exception as e:
                            # Log error but don't fail the whole process, correction can proceed without it
                            logging.error(f"Diarization step failed for {video_title}: {str(e)}", exc_info=True)
                            self.error_signal.emit(index, f"Diarization Failed: {str(e)}")
                            self.video_status_update.emit(index, "Warning", "Diarization Failed") # Show warning in table
                            diarization_result = [] # Ensure it's an empty list for the next step
                        finally:
                            self.clear_operation()
                    else:
                        logging.warning(f"Skipping diarization for {video_title} as pipeline is not available.")
                        self.video_status_update.emit(index, "Skipping Diarization", "")
                        diarization_result = []


                    # --- 5. Correct Transcript ---
                    if not self._is_running: break
                    self.set_operation(f"Correcting transcript for {video_title}")
                    self.video_status_update.emit(index, "Correcting", "")
                    try:
                        corrected_transcript = workflow_logic.correct_transcript(
                            raw_transcript,
                            diarization_result,
                            correction_model=self.correction_model
                        )
                        with open(corrected_transcript_path, "w", encoding='utf-8') as f:
                            f.write(corrected_transcript)
                        logging.info(f"Corrected transcript saved: {corrected_transcript_path}")
                    except Exception as e:
                         raise RuntimeError(f"Transcript correction failed: {str(e)}")
                    finally:
                        self.clear_operation()


                    # --- Mark as Complete ---
                    self.video_status_update.emit(index, "Complete", corrected_transcript_path)
                    processed_count = index + 1 # Update after successful completion

                except Exception as e:
                    # Catch errors specific to this video's processing pipeline
                    error_msg = f"Failed processing video {index} ({video_title}): {str(e)}"
                    logging.error(f"{error_msg}\n{traceback.format_exc()}")
                    self.error_signal.emit(index, error_msg)
                    self.video_status_update.emit(index, "Error", str(e)[:100]) # Show truncated error in table
                    # Continue to the next video

                finally:
                     self.clear_operation() # Ensure cleared even on error
                     # Update progress bar regardless of individual video success/failure
                     self.progress_update.emit(processed_count, total_videos)


            # --- End of Playlist ---
            if self._is_running:
                logging.info("Playlist processing finished.")
                self.status_update.emit("Playlist processing complete.")
            else:
                # If stopped, update progress bar to reflect last fully processed video
                self.progress_update.emit(processed_count, total_videos)
                logging.info("Playlist processing stopped by user.")
                self.status_update.emit("Playlist processing stopped.")

        except Exception as e:
            # Catch-all for unexpected errors during thread execution (e.g., outside loop)
            error_msg = f"An critical error occurred in the processing thread: {str(e)}"
            logging.critical(f"{error_msg}\n{traceback.format_exc()}")
            self.error_signal.emit(-1, error_msg)
            self.status_update.emit("Critical error occurred. See logs.")
        finally:
             self.clear_operation() # Final cleanup
             self.finished_signal.emit() # Emit finished signal


    def set_operation(self, description):
        """Safely set the current long-running operation description."""
        with self.current_operation_lock:
            self.current_operation = description
            logging.debug(f"Current operation set: {description}")

    def clear_operation(self):
        """Safely clear the current operation description."""
        with self.current_operation_lock:
            self.current_operation = ""
            # logging.debug("Current operation cleared.")

    def stop(self):
        """Signals the thread to stop gracefully."""
        logging.info("Stop signal received by ProcessingThread.")
        self._is_running = False
        # Optionally, add logic here to interrupt the *very specific* current operation
        # if possible (e.g., terminate ffmpeg subprocess), but this is complex.
        # For now, rely on checks within the loop.
        with self.current_operation_lock:
            op = self.current_operation
        if op:
            logging.info(f"Attempting to stop during operation: {op}")
        else:
            logging.info("Stop requested between operations.")

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("YouTube Playlist Advanced Processor")
        self.setGeometry(100, 100, 950, 750) # Slightly larger
        logging.info("Initializing MainWindow.")

        self.thread = None

        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_layout = QVBoxLayout(central_widget)

        # --- Input Area ---
        input_group = QWidget()
        input_layout = QGridLayout(input_group) # Use QGridLayout

        self.url_input = QLineEdit()
        self.url_input.setPlaceholderText("Enter YouTube Playlist URL")
        input_layout.addWidget(QLabel("Playlist URL:"), 0, 0)
        input_layout.addWidget(self.url_input, 0, 1, 1, 3) # Span 3 columns

        self.resolution_input = QLineEdit("720") # Default resolution
        self.resolution_input.setPlaceholderText("e.g., 1080, 720, best")
        input_layout.addWidget(QLabel("Max Video Res:"), 1, 0)
        input_layout.addWidget(self.resolution_input, 1, 1)

        # Updated: Model for Correction
        self.model_combo = QComboBox()
        # Add models suitable for correction/instruction following
        self.model_combo.addItems([
            "anthropic/claude-3-haiku-20240307", # Good balance
            "anthropic/claude-3-sonnet-20240229",
            "openai/gpt-4-turbo",
            "google/gemini-pro",
            "mistralai/mistral-large-latest"
        ])
        input_layout.addWidget(QLabel("Correction Model:"), 1, 2)
        input_layout.addWidget(self.model_combo, 1, 3)

        self.output_input = QLineEdit()
        self.output_input.setPlaceholderText("Select Output Directory")
        self.output_button = QPushButton("Browse...")
        self.output_button.clicked.connect(self.browse_output_directory)
        input_layout.addWidget(QLabel("Output Path:"), 2, 0)
        input_layout.addWidget(self.output_input, 2, 1, 1, 2)
        input_layout.addWidget(self.output_button, 2, 3)

        # Note: API Keys are now read from .env file by workflow_logic.py
        # Remove API Key input from GUI for better security practice
        # self.api_key_input = QLineEdit() ... (REMOVED)

        main_layout.addWidget(input_group)

        # --- Control Buttons ---
        button_layout = QHBoxLayout()
        self.start_button = QPushButton("Start Processing")
        self.start_button.clicked.connect(self.start_processing)
        button_layout.addWidget(self.start_button)

        self.stop_button = QPushButton("Stop Processing")
        self.stop_button.clicked.connect(self.stop_processing)
        self.stop_button.setEnabled(False)
        button_layout.addWidget(self.stop_button)
        main_layout.addLayout(button_layout)

        # --- Progress & Status ---
        self.progress_bar = QProgressBar()
        self.progress_bar.setTextVisible(True)
        self.progress_bar.setValue(0)
        self.progress_bar.setFormat("Waiting to start...")
        main_layout.addWidget(self.progress_bar)

        self.status_label = QLabel("Ready. Ensure OPENROUTER_API_KEY and HF_TOKEN (optional) are set in .env file or environment.")
        self.status_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        main_layout.addWidget(self.status_label)

        # --- Video Table ---
        self.video_table = QTableWidget(0, 3)
        self.video_table.setHorizontalHeaderLabels(["#", "Video Title", "Status/Result"])
        self.video_table.setColumnWidth(0, 40)  # Index column
        self.video_table.setColumnWidth(1, 400) # Title column
        self.video_table.horizontalHeader().setStretchLastSection(True) # Status column fills space
        main_layout.addWidget(self.video_table)

        # --- Error Log ---
        main_layout.addWidget(QLabel("Log / Errors:"))
        self.error_text = QTextEdit()
        self.error_text.setReadOnly(True)
        self.error_text.setMaximumHeight(150) # Limit height
        main_layout.addWidget(self.error_text)

        # Check for FFmpeg on startup
        QTimer.singleShot(100, self.initial_checks)

        logging.info("MainWindow UI setup complete.")

    def initial_checks(self):
        """Perform checks after UI is loaded."""
        if not workflow_logic.check_ffmpeg():
            QMessageBox.warning(self, "Dependency Check Failed",
                                "FFmpeg not found in system PATH. Audio extraction/conversion will fail. "
                                "Please install FFmpeg and ensure it's accessible.")
            self.status_label.setText("Error: FFmpeg not found!")
        if not os.getenv("OPENROUTER_API_KEY"):
             QMessageBox.warning(self, "Configuration Error",
                                "OPENROUTER_API_KEY not found in environment variables or .env file. "
                                "API calls will fail. Please set it.")
             self.status_label.setText("Error: OPENROUTER_API_KEY not set!")
        # Check pyannote status (optional, as workflow_logic handles it)
        if workflow_logic.diarization_pipeline is None:
             logging.warning("Pyannote pipeline failed to load during init.")
             self.error_text.append("Warning: Pyannote diarization pipeline failed to load. Diarization will be skipped. Check logs and HF token/model terms.")


    def browse_output_directory(self):
        directory = QFileDialog.getExistingDirectory(self, "Select Output Directory")
        if directory:
            self.output_input.setText(directory)
            logging.info(f"Output directory selected: {directory}")

    def start_processing(self):
        logging.info("Start processing button clicked.")
        playlist_url = self.url_input.text().strip()
        resolution = self.resolution_input.text().strip()
        output_path = self.output_input.text().strip()
        correction_model = self.model_combo.currentText()

        # --- Input Validation ---
        errors = []
        if not playlist_url: errors.append("Playlist URL is required.")
        # Basic URL check (can be improved)
        elif not playlist_url.startswith(("http://", "https://")):
             errors.append("Playlist URL seems invalid.")
        if not resolution: errors.append("Max Resolution is required.")
        elif not resolution.isdigit() and resolution.lower() != 'best':
             errors.append("Resolution must be a number (e.g., 720) or 'best'.")
        if not output_path: errors.append("Output Path is required.")
        elif not os.path.isdir(output_path):
             errors.append(f"Output Path is not a valid directory: {output_path}")
        if not os.getenv("OPENROUTER_API_KEY"): # Check env var directly now
             errors.append("OPENROUTER_API_KEY not found in environment. Please set it.")
        if not workflow_logic.check_ffmpeg(): # Re-check ffmpeg before starting
             errors.append("FFmpeg not found. Cannot proceed without it.")

        if errors:
            QMessageBox.warning(self, "Input Error", "\n".join(errors))
            logging.warning(f"Input validation failed: {errors}")
            return

        logging.info("Input validation passed.")
        # Clear previous results/errors
        self.video_table.setRowCount(0)
        self.error_text.clear()
        self.progress_bar.setValue(0)
        self.progress_bar.setFormat("Starting...")
        self.status_label.setText("Starting processing...")

        # Create and start the thread
        self.thread = ProcessingThread(playlist_url, resolution, output_path, correction_model)
        self.thread.status_update.connect(self.update_status)
        self.thread.video_status_update.connect(self.update_video_status)
        self.thread.error_signal.connect(self.handle_error)
        self.thread.progress_update.connect(self.update_progress)
        self.thread.finished_signal.connect(self.on_thread_finished) # Use custom finished signal

        try:
            self.thread.start()
            self.start_button.setEnabled(False)
            self.stop_button.setEnabled(True)
            logging.info("Processing thread started successfully.")
        except Exception as e:
            error_msg = f"Failed to start processing thread: {str(e)}"
            QMessageBox.critical(self, "Thread Start Error", error_msg)
            logging.critical(f"{error_msg}\n{traceback.format_exc()}")
            self.status_label.setText("Ready") # Reset status
            # Ensure buttons are reset if thread fails to start
            self.start_button.setEnabled(True)
            self.stop_button.setEnabled(False)

    def stop_processing(self):
        logging.info("Stop processing button clicked.")
        if self.thread and self.thread.isRunning():
            self.status_label.setText("Stopping... Please wait.")
            self.stop_button.setEnabled(False) # Disable stop button to prevent multiple clicks
            self.thread.stop()
            # Don't wait here, let the finished signal handle UI updates
        else:
             logging.warning("Stop clicked but no thread is running or already stopping.")

    def on_thread_finished(self):
        """Called when the background thread finishes (normally or via stop)."""
        logging.info("Processing thread finished signal received.")
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        # Final status is usually set by the thread itself before finishing
        # If status is still "Stopping...", update it
        if self.status_label.text().startswith("Stopping"):
             self.status_label.setText("Processing stopped.")
        self.thread = None # Release the thread object

    def update_status(self, message):
        self.status_label.setText(message)
        logging.debug(f"Status updated: {message}")

    def update_video_status(self, index, status, details):
        # Ensure table has enough rows
        current_rows = self.video_table.rowCount()
        if index >= current_rows:
            self.video_table.setRowCount(index + 1)

        # Set index number
        index_item = QTableWidgetItem(str(index + 1))
        index_item.setTextAlignment(Qt.AlignmentFlag.AlignCenter)
        self.video_table.setItem(index, 0, index_item)

        # Set video title only once
        title_item = self.video_table.item(index, 1)
        if not title_item:
             title = "Fetching title..." # Placeholder
             if self.thread and index < len(self.thread.video_list) and self.thread.video_list[index]:
                 title = self.thread.video_list[index].get('title', f'video_{index}')
             title_item = QTableWidgetItem(title)
             self.video_table.setItem(index, 1, title_item)

        # Update status/details column
        status_item = QTableWidgetItem(f"{status}: {details}" if details else status)
        if "Error" in status:
            status_item.setForeground(Qt.GlobalColor.red)
        elif "Warning" in status or "Skipping" in status:
             status_item.setForeground(Qt.GlobalColor.darkYellow)
        elif "Complete" in status:
             status_item.setForeground(Qt.GlobalColor.darkGreen)
        self.video_table.setItem(index, 2, status_item)
        logging.debug(f"Video {index+1} status updated: {status} - {details}")
        # Scroll to show the updated row?
        # self.video_table.scrollToItem(status_item, QAbstractItemView.ScrollHint.EnsureVisible)


    def handle_error(self, index, error_message):
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_msg = f"{timestamp} - Error (Video {index+1 if index >=0 else 'General'}): {error_message}"
        self.error_text.append(log_msg)
        logging.error(log_msg) # Already logged in thread, but good to have here too
        if index >= 0:
            # Update the specific video row status in the table
             if self.video_table.rowCount() > index:
                 error_item = QTableWidgetItem(f"Error: {error_message[:100]}...") # Truncate long errors
                 error_item.setForeground(Qt.GlobalColor.red)
                 self.video_table.setItem(index, 2, error_item) # Update status column
        else:
            # General error affecting the whole process
            QMessageBox.warning(self, "Processing Error", f"A general error occurred:\n{error_message}\n\nCheck the log for details.")
            self.status_label.setText("Error occurred. Check log.") # Update general status


    def update_progress(self, current, total):
        if total > 0:
            self.progress_bar.setMaximum(total)
            self.progress_bar.setValue(current)
            percentage = int((current / total) * 100)
            self.progress_bar.setFormat(f"{current}/{total} Videos Processed ({percentage}%)")
        else:
             # Reset progress bar if total is 0 (e.g., before fetching playlist)
             self.progress_bar.setMaximum(1) # Avoid division by zero for percentage
             self.progress_bar.setValue(0)
             self.progress_bar.setFormat("Fetching playlist info...")
        logging.debug(f"Progress updated: {current}/{total}")

    def closeEvent(self, event):
        """Ensure thread is stopped when closing the window."""
        logging.info("Close event triggered for main window.")
        if self.thread and self.thread.isRunning():
            reply = QMessageBox.question(self, 'Confirm Exit',
                                         "Processing is ongoing. Are you sure you want to stop and exit?",
                                         QMessageBox.StandardButton.Yes | QMessageBox.StandardButton.No,
                                         QMessageBox.StandardButton.No)
            if reply == QMessageBox.StandardButton.Yes:
                logging.info("User confirmed exit during processing. Stopping thread...")
                self.stop_processing()
                # Give thread a moment to acknowledge stop signal
                # self.thread.wait(2000) # Wait max 2 seconds (optional)
                event.accept()
            else:
                logging.info("User cancelled exit.")
                event.ignore()
        else:
            event.accept() # No thread running, close normally


if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    logging.info("Application started.")
    sys.exit(app.exec())

# --- END OF FILE main.py ---
```

**3. `requirements.txt`**

```txt
# --- START OF FILE requirements.txt ---
PyQt6>=6.0.0
yt-dlp>=2023.11.16 # Use a recent version
openai>=1.0.0     # For OpenRouter client
pyannote.audio>=3.1.1 # Use specific version aligned with model
torch>=2.0.0      # Check compatibility with pyannote.audio version
torchaudio>=2.0.0 # Check compatibility with torch
python-dotenv>=1.0.0
pytest>=7.0.0     # For running tests
pytest-mock>=3.0.0 # For mocking dependencies in tests
requests>=2.28    # Though not used directly in workflow, yt-dlp or others might need it

# Note: Ensure ffmpeg is installed separately and available in PATH.
# Note: For pyannote.audio, you might need to agree to model terms on Hugging Face Hub
#       and potentially set the HF_TOKEN environment variable.
# --- END OF FILE requirements.txt ---
```

**4. Unit Tests (`tests/test_workflow.py`)**

```python
# --- START OF FILE tests/test_workflow.py ---

import pytest
import os
import subprocess
from unittest.mock import patch, MagicMock, mock_open

# Mock environment variables before importing the module under test
@pytest.fixture(autouse=True)
def mock_env_vars(monkeypatch):
    monkeypatch.setenv("OPENROUTER_API_KEY", "sk-or-v1-fakekey")
    monkeypatch.setenv("HF_TOKEN", "hf_faketoken")
    # Import the module *after* setting env vars
    global workflow_logic
    import workflow_logic
    # Reload to ensure mocks are applied if already imported
    import importlib
    importlib.reload(workflow_logic)

# --- Fixtures ---

@pytest.fixture
def sample_diarization_result():
    return [
        {"speaker": "SPEAKER_00", "start": 0.500, "end": 2.800},
        {"speaker": "SPEAKER_01", "start": 3.100, "end": 5.200},
    ]

@pytest.fixture
def mock_openai_client():
    """Fixture to mock the OpenAI client used in workflow_logic"""
    with patch('workflow_logic.client', autospec=True) as mock_client:
        # Mock the transcription response
        mock_transcription = MagicMock()
        mock_transcription.text = "This is the raw transcript text."
        mock_client.with_options.return_value.audio.transcriptions.create.return_value = mock_transcription

        # Mock the chat completion (correction) response
        mock_choice = MagicMock()
        mock_choice.message.content = "SPEAKER_00: This is the corrected transcript.\nSPEAKER_01: With speaker labels."
        mock_completion = MagicMock()
        mock_completion.choices = [mock_choice]
        mock_client.with_options.return_value.chat.completions.create.return_value = mock_completion

        yield mock_client

@pytest.fixture
def mock_pyannote_pipeline():
    """Fixture to mock the pyannote pipeline"""
    # Mock the pipeline object loading and calling
    mock_pipeline_instance = MagicMock()

    # Define the mock output for pipeline call
    mock_turn_1 = MagicMock(start=0.5, end=2.8)
    mock_turn_2 = MagicMock(start=3.1, end=5.2)
    mock_track_1 = (mock_turn_1, '_', 'SPEAKER_00')
    mock_track_2 = (mock_turn_2, '_', 'SPEAKER_01')
    mock_diarization_result = MagicMock()
    mock_diarization_result.itertracks.return_value = iter([mock_track_1, mock_track_2])
    mock_diarization_result.labels.return_value = ['SPEAKER_00', 'SPEAKER_01'] # Needed for log message

    mock_pipeline_instance.return_value = mock_diarization_result # Mock the __call__ method

    # Patch the Pipeline.from_pretrained method AND the pipeline object in the module
    with patch('pyannote.audio.Pipeline.from_pretrained', return_value=mock_pipeline_instance) as mock_from_pretrained, \
         patch('workflow_logic.diarization_pipeline', mock_pipeline_instance) as mock_pipeline_obj:
             # Ensure the reloaded module uses the mocked instance
             workflow_logic.diarization_pipeline = mock_pipeline_instance
             yield mock_pipeline_instance


@pytest.fixture
def mock_ffmpeg():
    """Fixture to mock ffmpeg subprocess calls"""
    with patch('subprocess.run') as mock_run:
        # Mock the successful return for version check and conversion
        mock_process = MagicMock()
        mock_process.stderr = b""
        mock_process.stdout = b"ffmpeg version ..."
        mock_run.return_value = mock_process
        # Also patch the internal check flags for consistency
        with patch('workflow_logic._ffmpeg_checked', True), patch('workflow_logic._ffmpeg_present', True):
             yield mock_run

# --- Test Cases ---

def test_check_ffmpeg_found(mock_ffmpeg):
    """Test check_ffmpeg when ffmpeg is found"""
    assert workflow_logic.check_ffmpeg() is True
    mock_ffmpeg.assert_called_with(["ffmpeg", "-version"], check=True, capture_output=True)

def test_check_ffmpeg_not_found():
    """Test check_ffmpeg when ffmpeg is not found"""
    with patch('subprocess.run', side_effect=FileNotFoundError) as mock_run:
         # Reset checked flags for this specific test
         workflow_logic._ffmpeg_checked = False
         assert workflow_logic.check_ffmpeg() is False
         workflow_logic._ffmpeg_checked = False # Reset again after test
         mock_run.assert_called_with(["ffmpeg", "-version"], check=True, capture_output=True)


def test_extract_or_convert_audio_success(mock_ffmpeg, tmp_path):
    """Test successful audio conversion"""
    input_file = tmp_path / "input.mp4"
    output_file = tmp_path / "output.wav"
    input_file.touch() # Create dummy input file

    result_path = workflow_logic.extract_or_convert_audio(str(input_file), str(output_file))

    assert result_path == str(output_file)
    mock_ffmpeg.assert_any_call(["ffmpeg", "-version"], check=True, capture_output=True) # Check version call
    mock_ffmpeg.assert_called_with( # Check conversion call
        ["ffmpeg", "-i", str(input_file), "-vn", "-acodec", "pcm_s16le", "-ac", "1", "-ar", "16000", "-nostdin", "-y", str(output_file)],
        check=True, capture_output=True
    )

def test_extract_or_convert_audio_ffmpeg_fail(tmp_path):
    """Test audio conversion when ffmpeg command fails"""
    input_file = tmp_path / "input.mp4"
    output_file = tmp_path / "output.wav"
    input_file.touch()

    # Mock ffmpeg failure
    with patch('subprocess.run', side_effect=subprocess.CalledProcessError(1, "cmd", stderr=b"ffmpeg error")) as mock_run:
       # Ensure ffmpeg check initially passes
       with patch('workflow_logic.check_ffmpeg', return_value=True):
            with pytest.raises(RuntimeError, match="FFmpeg failed"):
                workflow_logic.extract_or_convert_audio(str(input_file), str(output_file))

def test_extract_or_convert_audio_ffmpeg_not_present(tmp_path):
    """Test audio conversion when ffmpeg is not installed"""
    input_file = tmp_path / "input.mp4"
    output_file = tmp_path / "output.wav"
    input_file.touch()

    with patch('workflow_logic.check_ffmpeg', return_value=False):
         # Reset internal flags for this test
         workflow_logic._ffmpeg_checked = False
         with pytest.raises(RuntimeError, match="FFmpeg is required"):
              workflow_logic.extract_or_convert_audio(str(input_file), str(output_file))
         workflow_logic._ffmpeg_checked = False # Reset after test


def test_transcribe_audio_success(mock_openai_client, tmp_path):
    """Test successful transcription"""
    audio_file = tmp_path / "audio.wav"
    # Use mock_open to simulate reading the file
    with patch("builtins.open", mock_open(read_data=b"dummy_audio_data")) as mock_file:
        transcript = workflow_logic.transcribe_audio(str(audio_file))

    assert transcript == "This is the raw transcript text."
    mock_file.assert_called_once_with(str(audio_file), "rb")
    mock_openai_client.with_options.return_value.audio.transcriptions.create.assert_called_once()

def test_transcribe_audio_api_error(mock_openai_client, tmp_path):
    """Test transcription failure due to API error"""
    audio_file = tmp_path / "audio.wav"
    # Simulate API error
    mock_openai_client.with_options.return_value.audio.transcriptions.create.side_effect = Exception("API Connection Error")

    with patch("builtins.open", mock_open(read_data=b"dummy_audio_data")):
        with pytest.raises(RuntimeError, match="Transcription failed: API Connection Error"):
            workflow_logic.transcribe_audio(str(audio_file))

def test_transcribe_audio_file_not_found(mock_openai_client):
    """Test transcription when audio file does not exist"""
    with pytest.raises(FileNotFoundError):
        workflow_logic.transcribe_audio("non_existent_file.wav")


def test_diarize_speakers_success(mock_pyannote_pipeline, tmp_path):
    """Test successful diarization"""
    audio_file = tmp_path / "audio.wav"
    audio_file.touch() # Create dummy file

    segments = workflow_logic.diarize_speakers(str(audio_file))

    assert len(segments) == 2
    assert segments[0] == {"speaker": "SPEAKER_00", "start": 0.500, "end": 2.800}
    assert segments[1] == {"speaker": "SPEAKER_01", "start": 3.100, "end": 5.200}
    mock_pyannote_pipeline.assert_called_once_with(str(audio_file), num_speakers=None)

def test_diarize_speakers_pipeline_fail(mock_pyannote_pipeline, tmp_path):
    """Test diarization when pipeline itself raises an error"""
    audio_file = tmp_path / "audio.wav"
    audio_file.touch()
    # Simulate pipeline call failure
    mock_pyannote_pipeline.side_effect = Exception("Pipeline error")

    # Should log error and return empty list, not raise exception
    segments = workflow_logic.diarize_speakers(str(audio_file))
    assert segments == []

def test_diarize_speakers_pipeline_not_loaded(tmp_path):
    """Test diarization when pipeline object is None"""
    audio_file = tmp_path / "audio.wav"
    audio_file.touch()
    # Ensure pipeline is None for this test
    with patch('workflow_logic.diarization_pipeline', None):
        segments = workflow_logic.diarize_speakers(str(audio_file))
        assert segments == []


def test_format_diarization_for_llm(sample_diarization_result):
    """Test formatting of diarization results"""
    formatted = workflow_logic.format_diarization_for_llm(sample_diarization_result)
    assert "Speaker Turns" in formatted
    assert "SPEAKER_00 0:00:00.500-0:00:02.800" in formatted
    assert "SPEAKER_01 0:00:03.100-0:00:05.200" in formatted

def test_format_diarization_for_llm_empty():
    """Test formatting when diarization result is empty"""
    formatted = workflow_logic.format_diarization_for_llm([])
    assert "Speaker diarization information is not available." in formatted


def test_correct_transcript_success(mock_openai_client, sample_diarization_result):
    """Test successful transcript correction"""
    raw = "this is the raw transcript text."
    correction_model = "test-correction-model"

    corrected = workflow_logic.correct_transcript(raw, sample_diarization_result, correction_model)

    assert corrected == "SPEAKER_00: This is the corrected transcript.\nSPEAKER_01: With speaker labels."
    # Check that the prompt was constructed correctly (simplified check)
    call_args = mock_openai_client.with_options.return_value.chat.completions.create.call_args
    prompt_content = call_args.kwargs['messages'][1]['content'] # User message
    assert raw in prompt_content
    assert "SPEAKER_00 0:00:00.500-0:00:02.800" in prompt_content # Check formatted diarization
    assert call_args.kwargs['model'] == correction_model


def test_correct_transcript_api_error(mock_openai_client, sample_diarization_result):
    """Test correction failure due to API error"""
    raw = "raw text"
    # Simulate API error
    mock_openai_client.with_options.return_value.chat.completions.create.side_effect = Exception("Correction API Error")

    with pytest.raises(RuntimeError, match="Transcript correction failed: Correction API Error"):
        workflow_logic.correct_transcript(raw, sample_diarization_result)

def test_correct_transcript_empty_raw():
    """Test correction when raw transcript is empty"""
    corrected = workflow_logic.correct_transcript("", [])
    assert corrected == ""

# --- END OF FILE tests/test_workflow.py ---
```

**5. Setup and Usage Instructions (README.md)**

```markdown
# --- START OF FILE README.md ---

# YouTube Playlist Advanced Processor

This application downloads videos from a YouTube playlist, extracts audio, performs transcription (Whisper via OpenRouter), speaker diarization (local pyannote.audio), and corrects/formats the transcript using an LLM (via OpenRouter).

## Features

*   GUI interface (PyQt6).
*   Processes entire YouTube playlists.
*   Downloads video (MP4) at specified max resolution.
*   Extracts audio and converts to 16kHz mono WAV using FFmpeg.
*   Transcribes audio using Whisper via OpenRouter.
*   Performs speaker diarization locally using `pyannote.audio`.
*   Corrects and formats transcript using a selected LLM via OpenRouter, incorporating speaker labels.
*   Saves results (video, WAV audio, raw transcript, diarization JSON, corrected transcript MD) in organized folders per video.
*   Real-time progress and status updates in the GUI.
*   Graceful start/stop functionality.
*   Robust error handling per video.
*   Uses environment variables for API keys.

## Prerequisites

1.  **Python:** Python 3.9 or higher recommended.
2.  **FFmpeg:** Must be installed and accessible in your system's PATH. FFmpeg is used for audio extraction and conversion. Download from [https://ffmpeg.org/](https://ffmpeg.org/).
3.  **Git:** (Optional) For cloning the repository.

## Setup

1.  **Clone the Repository (Optional):**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Create a Virtual Environment:**
    ```bash
    python -m venv venv
    # Activate the environment
    # Windows:
    venv\Scripts\activate
    # macOS/Linux:
    source venv/bin/activate
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Note: Installing `torch` and `torchaudio` might take time. Ensure you have sufficient disk space.*

4.  **Configure API Keys and Tokens:**
    *   Create a file named `.env` in the project's root directory (where `main.py` is located).
    *   Add your OpenRouter API key to the `.env` file:
        ```dotenv
        OPENROUTER_API_KEY="sk-or-v1-your-openrouter-api-key-here"
        ```
    *   **(Optional but Recommended for `pyannote.audio`)** Add your Hugging Face Hub token (if needed for the diarization model, e.g., `pyannote/speaker-diarization-3.1`). You need to accept the model's terms on the Hugging Face website first. Get a token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).
        ```dotenv
        HF_TOKEN="hf_your_huggingface_token_here"
        ```
        Alternatively, you can log in using `huggingface-cli login` in your terminal before running the application.

5.  **Accept `pyannote.audio` Model Terms:**
    *   You MUST manually visit the Hugging Face Hub pages for the models used by the `pyannote/speaker-diarization-3.1` pipeline (check the `pyannote.audio` documentation or error messages for specific models like `pyannote/segmentation-3.0` and `speechbrain/speaker-recognition-ecapa-tdnn`) and accept their terms of use while logged into your Hugging Face account. The application cannot do this for you. Failure to do so will likely result in errors when loading the diarization pipeline.

## Running the Application

1.  Ensure your virtual environment is active.
2.  Ensure the `.env` file is configured correctly.
3.  Run the main script:
    ```bash
    python main.py
    ```
4.  Use the GUI to:
    *   Enter the YouTube Playlist URL.
    *   Set the maximum video resolution.
    *   Choose the LLM for transcript correction.
    *   Select the main output directory.
    *   Click "Start Processing".

## Running Tests

1.  Ensure your virtual environment is active.
2.  Install test dependencies (already included in `requirements.txt`):
    ```bash
    pip install pytest pytest-mock
    ```
3.  Run pytest from the project's root directory:
    ```bash
    pytest
    ```

## Important Notes

*   **API Costs:** Using OpenRouter incurs costs based on the models selected (Whisper, Correction LLM) and the amount of processing. Monitor your usage and costs on the OpenRouter website.
*   **Processing Time:** Processing can be lengthy, depending on the number/duration of videos, network speed, API response times, and your computer's performance (especially for local diarization).
*   **`pyannote.audio`:** Diarization quality can vary. It requires downloading models on the first run, which can take time and requires internet access. A GPU significantly speeds up diarization.
*   **Error Handling:** The application attempts to handle errors per video, allowing the rest of the playlist to process. Check the log/error panel in the GUI and the console output for details if issues occur.
*   **Stopping:** Clicking "Stop" signals the current video to finish its *current major step* (Download, Convert, Transcribe, Diarize, Correct) before stopping. It may not be instantaneous.

# --- END OF FILE README.md ---
```

This comprehensive response provides the refactored code, integrated workflow, tests, requirements, and setup instructions based on the assumption you want to merge the advanced workflow into the GUI application. Remember to handle your API keys securely using the `.env` file and remove any hardcoded keys. You will also need to install `ffmpeg` and potentially manage Hugging Face authentication for `pyannote.audio`.
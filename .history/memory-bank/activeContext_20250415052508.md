# Active Context

## Current Focus

- **OpenRouter-Aligned LLM Model Selection and Workflow:**
  - Transcription: Performed using the official OpenAI Whisper API (`whisper-1`) with `OPENAI_API_KEY` (not via OpenRouter, due to lack of endpoint).
  - Sentence splitting and cleanup: SpaCy (required).
  - LLM correction and summarization: User-selectable from all OpenRouter LLMs, with live price calculation and advanced model selection (search, variants, provider routing, error handling, model details).
  - Pseudo-diarization: After transcription, the transcript (with or without timestamps) is sent to an OpenRouter LLM, which infers and assigns speaker labels based on context and dialogue cues. No Hugging Face/pyannote or audio-based diarization is used.
  - Output: Per-video folders with audio, raw transcript, LLM-labeled transcript, summary, metadata (CSV/JSON), and HTML transcript reader.
  - UI: Correction and summarization dropdowns are populated live from OpenRouter, support search/filter, show model details (provider, context, price, variants, tool calling, structured outputs), and allow advanced selection (variants, provider routing).
  - Robust error handling: All API/network/cache errors are surfaced in the GUI, with clear user feedback and troubleshooting steps.

**Technical Note:**  
OpenRouter does not provide a Whisper (audio transcription) API endpoint or built-in diarization. The pipeline uses the official OpenAI API for transcription, OpenRouter for all LLM-based tasks (including pseudo-diarization), and does not require Hugging Face or pyannote.audio. Both `OPENAI_API_KEY` and `OPENROUTER_API_KEY` must be set in the environment.

## Recent Changes

- Model fetching, filtering, and caching now strictly follow OpenRouter API best practices.
- GUI supports advanced model selection, search/filter, and error handling.
- All documentation, memory bank, and planning files updated to reflect OpenRouter's unified API and advanced usage.
- All code, UI, and tests are robust, user-friendly, and ready for production use with OpenRouter.

## Next Steps

- Continue to test the full pipeline with real data and large model lists.
- Monitor for new OpenRouter features (e.g., new model types, advanced routing) and update the workflow as needed.
- Maintain documentation and memory bank alignment with OpenRouter's evolving API and best practices.
- Consider further enhancements:
  - Exporting combined metadata.
  - More granular progress/status reporting.
  - Additional output formats or downstream integrations.

## Active Decisions & Considerations

- No user options for transcription or sentence splitting models.
- All LLM steps (correction, summarization) are user-selectable from OpenRouter, with live pricing and advanced selection.
- Diarization is always performed using pyannote.audio locally.
- Output is always per-video, with all assets grouped and clearly named.
- All error handling and user feedback is surfaced in the GUI, not just logs.

## Patterns & Preferences

- PEP 8 compliance for all Python code.
- Modular, extensible architecture.
- Robust error handling and logging.
- User experience focused on clarity, transparency of cost, and ease of use.
- Strict alignment with OpenRouter's API, model selection, and advanced usage documentation.

## Project Insights

- The project is now fully aligned with OpenRouter's unified API, model selection, and advanced usage patterns.
- All code, UI, and documentation are robust, user-friendly, and ready for production use.

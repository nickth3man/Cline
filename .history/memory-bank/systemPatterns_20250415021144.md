# System Patterns

## System Architecture

- The workflow is organized around per-video folders, each named after the video title.
- All processing (video download, audio extraction, transcription, diarization, correction, summarization) is performed in-place within each video folder.
- A centralized SQLite database tracks all video metadata, file paths, transcripts, summaries, and processing status.
- Export functionality is integrated, allowing users to extract data from the database to CSV or JSON Lines format via both CLI and GUI.

## Key Technical Decisions

- Use yt-dlp to download videos, extract audio, and save metadata (`--write-info-json`).
- Use OpenAI (via OpenRouter) for transcription (Whisper), transcript correction (e.g., Claude 3, GPT-4), and summarization.
- Use `pyannote.audio` for speaker diarization (optional, requires setup).
- Store all outputs (videos, metadata `.info.json`, audio, `corrected_transcript.md`, `summary.md`) in per-video folders.
- **Centralized Database:** Use SQLite (`database_manager.py`) to store metadata, file paths, transcript content, summary content, and processing status for all videos.
- All data management (status, retry, delete, export) is performed via the database, accessible from both CLI and GUI.
- Export logic is streaming/chunked to efficiently handle large datasets.

## Design Patterns in Use

- **Per-entity grouping (Filesystem):** All media/text assets for a single video are colocated in a folder.
- **Centralized Index (Database):** A SQLite database acts as the primary index and data store for metadata and text content, enabling efficient querying, status tracking, and export.
- **Template-based naming:** Output files use consistent naming conventions.
- **Modular Components:** Separate modules for workflow logic (`workflow_logic.py`), database interaction (`database_manager.py`), CLI (`transcription_workflow.py`), and GUI (`src/main.py`).
- **Streaming Export:** Export logic is designed to handle large datasets without high memory usage.

## Component Relationships

- `transcription_workflow.py` (CLI) and `src/main.py` (GUI):
    - Instantiate `DatabaseManager` (which calls `initialize_database`).
    - **Pipeline Execution:**
        - Call `download_and_extract_audio` (CLI) or equivalent logic in `ProcessingThread` (GUI).
            - This logic calls `yt-dlp` and `DatabaseManager.add_or_update_video` (with initial info and status 'pending_download').
            - Updates DB status after download attempt ('downloaded' or 'error_*') via `DatabaseManager.add_or_update_video`.
        - Call `process_downloaded_audio` (CLI) or equivalent logic in `ProcessingThread` (GUI).
            - Queries DB for videos with status 'downloaded' via `DatabaseManager.get_videos_by_status`.
            - Calls `workflow_logic.process_audio_file` for each.
            - Calls `DatabaseManager.add_transcript` and `add_summary` on success.
            - Calls `DatabaseManager.add_or_update_video` or `DatabaseManager.update_video_status` to update status ('processed' or 'error_*').
    - **Data Management:**
        - Both CLI (via args) and GUI (via "Manage Data" tab) use `DatabaseManager` methods:
            - `get_all_video_details()`: To display video lists/tables.
            - `get_videos_by_status()`: To list errors or find items to retry.
            - `get_video_details()`: To show detailed view.
            - `get_video_paths()`: To find associated files for deletion.
            - `update_video_status()`: To mark videos for retry.
            - `delete_video()`: To remove database records (file deletion handled in CLI/GUI logic after confirmation).
    - **Data Export:**
        - CLI (`transcription_workflow.py`) uses arguments (`--export`, `--ids`, `--status_filter`) which call corresponding `DatabaseManager` methods (`export_to_csv`, `export_to_json`).
        - GUI (`src/main.py`) uses a context menu action ("Export Selected...") on the "Manage Data" tab. This action triggers an `ExportThread` which calls the same `DatabaseManager` export methods, passing selected video IDs.
        - Both interfaces use streaming/chunked export logic in `DatabaseManager` to efficiently handle large datasets and avoid memory issues.
- `workflow_logic.py`: Contains core audio processing functions (transcription, diarization, correction, summarization). Does not interact directly with the database.
- `database_manager.py`: Encapsulates all SQLite interactions (connection, schema, CRUD, querying, helper methods, dataclasses). Provides the public interface for database operations used by CLI and GUI, including the core data export logic (`export_to_csv`, `export_to_json`) with streaming/chunking.

## Critical Implementation Paths

- **Initialization:** `initialize_database` creates tables if they don't exist.
- **Download Phase:**
    - Fetch playlist info (`yt-dlp --dump-json`).
    - For each video:
        - Add/update video record in DB (status 'pending_download').
        - Run `yt-dlp` to download video, metadata, extract audio.
        - Update video record in DB with status ('downloaded', 'error_*') and file paths (`metadata_json_path`, `audio_wav_path`).
- **Processing Phase:**
    - Query DB for videos with status 'downloaded'.
    - For each 'downloaded' video:
        - Update status to 'processing' in DB.
        - Call `workflow_logic.process_audio_file`.
        - If successful:
            - Read corrected transcript and summary content.
            - Add transcript and summary records to DB (including content and file path).
            - Update video status to 'processed' in DB.
        - If failed:
            - Update video status to 'error_processing' or similar in DB.
- **Export Phase:**
    - CLI: User runs with `--export {csv|jsonl}` and optional `--ids` or `--status_filter` to select which videos to export. The CLI calls `DatabaseManager.export_to_csv` or `export_to_json`, which stream results to the output file.
    - GUI: User selects videos in the "Manage Data" tab and chooses "Export Selected..." from the context menu. This triggers an `ExportThread` that calls the same export methods in `DatabaseManager`.
    - Both: Export logic is streaming/chunked to support large exports without high memory usage.
- Improved error handling and logging are implemented throughout, including writing to per-video `error.log` files and updating DB status on error.

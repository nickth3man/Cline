# Tech Context

## Technologies Used

- **yt-dlp**: For downloading videos, extracting audio, and saving metadata.
- **Python**: Main workflow scripting and orchestration (`transcription_workflow.py`, `src/main.py`).
- **SQLite**: Central database for tracking metadata, status, and text content (`database_manager.py`).
- **OpenAI (via OpenRouter)**: For transcription (Whisper) and transcript correction (e.g., Claude 3, GPT-4). Summarization model (e.g., GPT-3.5-turbo) also used if enabled.
- **pyannote.audio**: For speaker diarization (optional, requires setup).
- **PyQt6**: For the GUI (`src/main.py`).
- **dotenv**: For environment variable management.
- **Standard Python libraries**: os, subprocess, logging, re, datetime, json, sqlite3, csv.

## Development Setup

- Python 3.8+ recommended.
- yt-dlp must be installed and available in the system PATH.
- OpenAI API key (via OpenRouter) must be set in the environment.
- All scripts are run from the project root; outputs are organized under the specified working directory.

## Technical Constraints

- Video titles with special characters or duplicates may require sanitization for filesystem compatibility.
- The workflow assumes a stable internet connection for downloading and API calls.
- Large playlists or long videos may require additional disk space and processing time.

## Dependencies

- yt-dlp
- openai
- python-dotenv
- PyQt6
- pyannote.audio (optional, for diarization)
- torch (optional, dependency for pyannote.audio)
# Note: sqlite3 is built-in

## Tool Usage Patterns

- `yt-dlp` is called via `subprocess` for fetching playlist info (`--dump-json`), video download (`--write-info-json`), and audio extraction.
- `database_manager.py` (now a `DatabaseManager` class) handles all SQLite operations (initialization, CRUD, querying) using helper methods and dataclasses.
- **Pipeline Execution:** Both CLI and GUI workflows call `DatabaseManager` methods (`add_or_update_video`, `add_transcript`, `update_video_status`) during the download and processing phases to store results and track status.
- **Data Management:**
    - The CLI (`transcription_workflow.py`) uses arguments (`--status`, `--list-errors`, `--view`, `--retry`, `--delete`) which call corresponding `DatabaseManager` methods (`get_all_video_details`, `get_videos_by_status`, `get_video_details`, `get_video_paths`, `update_video_status`, `delete_video`).
    - The GUI (`src/main.py`) uses a `QTableView` with a custom `QAbstractTableModel` on the "Manage Data" tab. Actions (Refresh, Retry All, context menu) trigger calls to the same `DatabaseManager` methods used by the CLI for viewing, retrying, and deleting data.
- **Data Export:**
    - The CLI uses new arguments (`--export`, `--ids`, `--status`) to trigger calls to `DatabaseManager.export_to_csv` or `DatabaseManager.export_to_json`.
    - The GUI uses a context menu action ("Export Selected...") which triggers an `ExportThread`. The thread calls the same `DatabaseManager` export methods using selected IDs obtained from the table view.
    - Both interfaces use streaming/chunked export logic in `DatabaseManager` to handle large datasets efficiently and avoid memory constraints.
- Core audio processing (transcription, diarization, correction) is handled by `workflow_logic.process_audio_file`.
- `pyannote.audio` pipeline is loaded in `workflow_logic.py` for diarization if available.
- OpenAI API (via OpenRouter) is used for transcription (Whisper) and correction (configurable model) within `workflow_logic.py`.
- The GUI (`src/main.py`) uses a `QThread` to run the download and processing steps, interacting with the database via the `DatabaseManager` instance.
- Improved error handling and logging are implemented throughout, including DB status updates.
# Tech Context

## Technologies Used

- **yt-dlp**: For downloading all available video resolutions and extracting audio from YouTube playlists.
- **Python**: Main workflow scripting and orchestration.
- **OpenAI (via OpenRouter)**: For transcription (Whisper) and summarization (GPT-3.5-turbo).
- **dotenv**: For environment variable management.
- **Standard Python libraries**: os, subprocess, logging, re, etc.

## Development Setup

- Python 3.8+ recommended.
- yt-dlp must be installed and available in the system PATH.
- OpenAI API key (via OpenRouter) must be set in the environment.
- All scripts are run from the project root; outputs are organized under the specified working directory.

## Technical Constraints

- Video titles with special characters or duplicates may require sanitization for filesystem compatibility.
- The workflow assumes a stable internet connection for downloading and API calls.
- Large playlists or long videos may require additional disk space and processing time.

## Dependencies

- yt-dlp
- openai
- python-dotenv

## Tool Usage Patterns

- yt-dlp is called via subprocess for both video and audio extraction, using template-based output paths.
- Directory walking is used to process all .wav files for transcription and all transcript.md files for summarization.
- All outputs are colocated with their source video/audio for maintainability, and errors are logged to error.log in each video folder.

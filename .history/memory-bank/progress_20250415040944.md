# Progress

## Current Status

- **OpenRouter-Aligned Workflow Fully Implemented:**
  - Transcription: Fixed to `openai/whisper-large-v3` via OpenRouter.
  - Sentence Splitting: SpaCy (required).
  - LLM Correction & Summarization: User-selectable from all OpenRouter LLMs, with live price calculation, advanced model selection (search, variants, provider routing, model details), and robust error handling.
  - Diarization: Always performed using `pyannote.audio` locally (requires Hugging Face token and setup).
  - Output: Per-video folders with all assets (audio, transcripts, summary, metadata, diarization, HTML reader).
  - UI: Live dropdowns for LLM steps, real-time cost display, search/filter, model details, variants, provider routing, and error feedback.
  - All documentation, memory bank, and planning files are up to date and aligned with OpenRouter's API and best practices.
  - All code, UI, and tests are robust, user-friendly, and ready for production use.

## Remaining Tasks

- Continue to test the full pipeline with real data and large model lists.
- Monitor for new OpenRouter features (e.g., new model types, advanced routing) and update the workflow as needed.
- Maintain documentation and memory bank alignment with OpenRouter's evolving API and best practices.
- Consider further enhancements:
  - Exporting combined metadata.
  - More granular progress/status reporting.
  - Additional output formats or downstream integrations.

## Known Issues

- pyannote.audio requires local setup and a Hugging Face token for gated models.
- OpenRouter API costs for transcription and LLM steps depend on audio length and model selection.
- SpaCy must be installed and available for sentence splitting.

## Evolution of Decisions

- Moved from optional diarization and fixed LLMs to a fully user-selectable LLM workflow with live pricing, advanced model selection, and required diarization.
- All planning and documentation now strictly aligned with OpenRouter's API, model selection, and advanced usage documentation.

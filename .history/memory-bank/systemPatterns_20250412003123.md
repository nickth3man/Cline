# System Patterns

## System Architecture

- The workflow is organized around per-video folders, each named after the video title.
- All processing (video download, audio extraction, transcription, summarization) is performed in-place within each video folder.

## Key Technical Decisions

- Use yt-dlp to download videos, extract audio, and save metadata (`--write-info-json`).
- Use OpenAI (via OpenRouter) for transcription (Whisper) and transcript correction (e.g., Claude 3, GPT-4).
- Use `pyannote.audio` for speaker diarization (optional, requires setup).
- Store all outputs (videos, metadata `.info.json`, audio, `corrected_transcript.md`, `summary.md`) in per-video folders.
- **Centralized Database:** Use SQLite (`database_manager.py`) to store metadata, file paths, transcript content, and processing status for all videos.
- Walk the directory tree or query the database to process relevant files.
- Centralize core processing logic (transcription, diarization, correction) in `workflow_logic.py`.
- Centralize database interactions in `database_manager.py`.

## Design Patterns in Use

- **Per-entity grouping (Filesystem):** All media/text assets for a single video are colocated in a folder.
- **Centralized Index (Database):** A SQLite database acts as the primary index and data store for metadata and text content, enabling efficient querying and status tracking.
- **Template-based naming:** Output files use consistent naming conventions.
- **Modular Components:** Separate modules for workflow logic (`workflow_logic.py`), database interaction (`database_manager.py`), CLI (`transcription_workflow.py`), and GUI (`src/main.py`).

## Component Relationships

- `transcription_workflow.py` (CLI) and `src/main.py` (GUI):
    - Instantiate `DatabaseManager` (which calls `initialize_database`).
    - **Pipeline Execution:**
        - Call `download_and_extract_audio` (CLI) or equivalent logic in `ProcessingThread` (GUI).
            - This logic calls `yt-dlp` and `DatabaseManager.add_or_update_video` (with initial info and status 'pending_download').
            - Updates DB status after download attempt ('downloaded' or 'error_*') via `DatabaseManager.add_or_update_video`.
        - Call `process_downloaded_audio` (CLI) or equivalent logic in `ProcessingThread` (GUI).
            - Queries DB for videos with status 'downloaded' via `DatabaseManager.get_videos_by_status`.
            - Calls `workflow_logic.process_audio_file` for each.
            - Calls `DatabaseManager.add_transcript` on success.
            - Calls `DatabaseManager.add_or_update_video` or `DatabaseManager.update_video_status` to update status ('processed' or 'error_*').
    - **Data Management (New):**
        - Both CLI (via args) and GUI (via "Manage Data" tab) use `DatabaseManager` methods:
            - `get_all_video_details()`: To display video lists/tables.
            - `get_videos_by_status()`: To list errors or find items to retry.
            - `get_video_details()`: To show detailed view.
            - `get_video_paths()`: To find associated files for deletion.
            - `update_video_status()`: To mark videos for retry.
            - `delete_video()`: To remove database records (file deletion handled in CLI/GUI logic after confirmation).
    - **Data Export (New):**
        - CLI (`transcription_workflow.py`) uses new arguments (`--export`, `--ids`, `--status`) which call corresponding `DatabaseManager` methods (`export_to_csv`, `export_to_json`).
        - GUI (`src/main.py`) uses a context menu action ("Export Selected...") on the "Manage Data" tab. This action triggers an `ExportThread` which calls the same `DatabaseManager` export methods, passing selected video IDs.
        - Both interfaces use streaming/chunked export logic in `DatabaseManager` to efficiently handle large datasets and avoid memory issues.
- `workflow_logic.py`: Contains core audio processing functions (transcription, diarization, correction). Does not interact directly with the database.
- `database_manager.py`: Encapsulates all SQLite interactions (connection, schema, CRUD, querying, helper methods, dataclasses). Provides the public interface for database operations used by CLI and GUI, **including the core data export logic (`export_to_csv`, `export_to_json`) with streaming/chunking**.

## Critical Implementation Paths

- **Initialization:** `initialize_database` creates tables if they don't exist.
- **Download Phase:**
    - Fetch playlist info (`yt-dlp --dump-json`).
    - For each video:
        - Add/update video record in DB (status 'pending_download').
        - Run `yt-dlp` to download video, metadata, extract audio.
        - Update video record in DB with status ('downloaded', 'error_*') and file paths (`metadata_json_path`, `audio_wav_path`).
- **Processing Phase:**
    - Query DB for videos with status 'downloaded'.
    - For each 'downloaded' video:
        - Update status to 'processing' in DB.
        - Call `workflow_logic.process_audio_file`.
        - If successful:
            - Read corrected transcript content.
            - Add transcript record to DB (including content and file path).
            - Update video status to 'processed' in DB.
        - If failed:
            - Update video status to 'error_processing' or similar in DB.
- (Optional) **Summarization Phase:** Query DB for 'processed' videos without summaries, generate summaries, add to DB.
- **Export Phase (New):** 
    - CLI: User runs with `--export {csv|jsonl}` and optional `--ids` or `--status` to select which videos to export. The CLI calls `DatabaseManager.export_to_csv` or `export_to_json`, which stream results to the output file.
    - GUI: User selects videos in the "Manage Data" tab and chooses "Export Selected..." from the context menu. This triggers an `ExportThread` that calls the same export methods in `DatabaseManager`.
    - Both: Export logic is streaming/chunked to support large exports without high memory usage.
- Improved error handling and logging are implemented throughout, including writing to per-video `error.log` files and updating DB status on error.
# System Patterns

## System Architecture

- The workflow is organized around per-video folders, each named after the video title.
- All processing (video download, audio extraction, transcription, summarization) is performed in-place within each video folder.

## Key Technical Decisions

- Use yt-dlp to download all available video resolutions and extract audio.
- Use OpenAI (via OpenRouter) for transcription and summarization.
- Store all outputs (videos, audio, transcript.md, summary.md) in the same folder for each video.
- Walk the directory tree to process all relevant files, ensuring robustness to batch operations.

## Design Patterns in Use

- **Per-entity grouping:** All assets for a single video are colocated, reducing lookup and management complexity.
- **Template-based naming:** Output files use yt-dlp's template variables for consistent, collision-resistant naming.
- **In-place processing:** Transcription and summarization are performed in the same folder as the source files, avoiding scattered outputs.

## Component Relationships

- The download_and_extract_audio function is responsible for all video and audio asset creation and organization.
- The transcribe_audio_files and summarize_transcriptions functions operate recursively on the audio directory, ensuring all outputs are grouped per video.
- The workflow is modular, allowing for additional processing steps (e.g., metadata extraction, diarization) to be added as needed, and includes robust error handling and logging.

## Critical Implementation Paths

- For each video in the playlist:
  - Download all available video resolutions into (output_dir)/(video title)/(video_title)_(resolution).mp4
  - Extract audio as (output_dir)/(video title)/video_title.wav
  - Transcribe audio to (output_dir)/(video title)/transcript.md
  - Summarize transcript to (output_dir)/(video title)/summary.md
  - Any errors are logged to (output_dir)/(video title)/error.log
- A summary of all errors is printed at the end of the run.

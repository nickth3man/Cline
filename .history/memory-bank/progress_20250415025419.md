# Progress

## Current Status

- **New Plan Approved and In Progress:**
  - Transcription: Fixed to `openai/whisper-large-v3` via OpenRouter.
  - Sentence Splitting: SpaCy (required).
  - LLM Correction & Summarization: User-selectable from all OpenRouter LLMs, with live price calculation.
  - Diarization: Always performed using `pyannote.audio` locally (requires Hugging Face token and setup).
  - Output: Per-video folders with all assets (audio, transcripts, summary, metadata, diarization, HTML reader).
  - UI: Live dropdowns for LLM steps, real-time cost display.
- **All previous planning and options are now superseded by this workflow.**

## Remaining Tasks

- Update all memory bank and planning documents to reflect the new plan.
- Refactor code and UI to:
  - Enforce fixed models for transcription and sentence splitting.
  - Add live OpenRouter LLM dropdowns and price calculation for correction and summarization.
  - Integrate pyannote.audio for diarization, with setup checks and error handling.
  - Ensure output structure and logs match the plan.
- Test the full pipeline for robustness, usability, and cost transparency.

## Known Issues

- pyannote.audio requires local setup and a Hugging Face token for gated models.
- OpenRouter API costs for transcription and LLM steps depend on audio length and model selection.
- SpaCy must be installed and available for sentence splitting.

## Evolution of Decisions

- Moved from optional diarization and fixed LLMs to a fully user-selectable LLM workflow with live pricing and required diarization.
- All planning and documentation now aligned with the approved implementation path.
